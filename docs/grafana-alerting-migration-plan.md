# Plan: Migrate Slack Alerting from Alertmanager to Grafana Alerting

## Context

Alertmanager's Slack notifications have three problems:
1. **Literal `\n`** in messages (YAML single-quote escaping bug in the template)
2. **Internal K8s URL** as the link (`kube-prometheus-stack-alertmanager.monitoring:9093`) — inaccessible outside the cluster
3. **No actionable links** — can't silence, view dashboards, or acknowledge from Slack

Grafana Alerting provides richer Slack integration: proper formatting, links to `grafana.sharmamohit.com` (actually accessible), silence/view buttons, and dashboard deep-links.

Additionally, SmartDiskTemperatureHigh (55C threshold) is flapping on NVMe drives in mini PCs — normal operating temp is 50-60C. Raising to 60C.

## Architecture

**Before:**
```
Prometheus rules → Alertmanager → Slack (bad formatting)
                                → Home Assistant webhook
```

**After:**
```
Grafana alert rules → Grafana contact point → Slack (rich formatting)
Prometheus rules    → Alertmanager           → Home Assistant webhook
```

- Grafana evaluates its own rules against the Thanos datasource (uid: `thanos`)
- Alertmanager keeps the HA webhook (unchanged)
- Alertmanager's Slack receiver is removed
- Duplicate evaluation (Prometheus + Grafana) is acceptable for a homelab
- Thresholds must be kept in sync between Prometheus rules (HA) and Grafana rules (Slack) — add a comment block in the YAML noting this

## Pre-step: Install Grafana MCP Server

Add `mcp-grafana` to Claude Code's MCP config for verification.

1. User creates a Grafana service account + token at `https://grafana.sharmamohit.com/org/serviceaccounts`
   - Role: **Viewer** (read-only — sufficient for listing alert rules, contact points, policies)
   - Set token expiration to 90 days
2. Add to `~/.claude/mcp.json` or project `.mcp.json`:
```json
{
  "mcpServers": {
    "grafana": {
      "command": "npx",
      "args": ["-y", "mcp-grafana"],
      "env": {
        "GRAFANA_URL": "https://grafana.sharmamohit.com",
        "GRAFANA_API_KEY": "<service-account-token>"
      }
    }
  }
}
```
3. Add `mcp.json` and `.mcp.json` to `.gitignore` to prevent accidental token commits.

## Changes

### Phase 1: Add Grafana Alerting (keep Alertmanager Slack active)

Deploy Grafana alerting alongside the existing Alertmanager Slack receiver first. Verify the Grafana pipeline works end-to-end before removing Alertmanager's Slack config. This avoids any alerting gap.

#### 1a. Set `GF_SERVER_ROOT_URL` for correct Slack links

**File:** `kubernetes/apps/argocd-apps/apps/kube-prometheus-stack.yaml`

Without this, all silence/alert links in Slack point to `http://localhost:3000`. Add:

```yaml
grafana:
  grafana.ini:
    server:
      root_url: https://grafana.sharmamohit.com
    unified_alerting:
      enabled: true
    alerting:
      enabled: false  # disable legacy alerting engine
```

#### 1b. Inject Slack webhook URL into Grafana pod

Use the `env` key with `valueFrom` syntax (not `envValueFrom` — the Grafana subchart requires this form):

```yaml
grafana:
  env:
    GF_SLACK_WEBHOOK_URL:
      valueFrom:
        secretKeyRef:
          name: alertmanager-slack-webhook
          key: slack-webhook-url
```

This reuses the existing SOPS-encrypted secret without creating a new one.

#### 1c. Add `grafana.alerting` section

Add the full alerting provisioning under `grafana.alerting`. This creates files in `/etc/grafana/provisioning/alerting/` inside the Grafana pod.

**Notification template** (`templates.yaml`):
```yaml
grafana:
  alerting:
    templates.yaml:
      apiVersion: 1
      templates:
        - orgId: 1
          name: homelab-slack
          template: |
            {{ define "slack.homelab.title" -}}
            {{ if eq .Status "firing" }}:fire:{{ else }}:white_check_mark:{{ end }} {{ .CommonLabels.alertname }}
            {{- end }}

            {{ define "slack.homelab.text" -}}
            {{ range .Alerts -}}
            *{{ .Labels.severity | toUpper }}* — {{ .Labels.instance }}
            {{ .Annotations.summary }}
            :chart_with_upwards_trend: <https://grafana.sharmamohit.com/alerting/list|View Alerts>  {{ if .SilenceURL }}:no_bell: <{{ .SilenceURL }}|Silence>{{ end }}
            {{ end -}}
            {{- end }}
```

> **Note:** `.DashboardURL` is only populated when alert rules are explicitly linked to a dashboard panel via `dashboardUid`/`panelId`. Since our rules are standalone, we use a static link to the alerting list page instead. `.SilenceURL` is auto-generated by Grafana for firing alerts (requires `root_url` from step 1a).

**Contact point** (`contactpoints.yaml`):
```yaml
    contactpoints.yaml:
      apiVersion: 1
      contactPoints:
        - orgId: 1
          name: slack-homelab
          receivers:
            - uid: slack-homelab-receiver
              type: slack
              disableResolveMessage: false
              settings:
                title: '{{ template "slack.homelab.title" . }}'
                text: '{{ template "slack.homelab.text" . }}'
              secureSettings:
                url: $GF_SLACK_WEBHOOK_URL
```

> **Security note:** The webhook URL goes in `secureSettings` (not `settings`). This instructs Grafana to encrypt the value at rest and redact it in API responses. Using `settings.url` would expose the webhook via the `/api/v1/provisioning/contact-points` endpoint to any Editor user.

**Notification policy** (`policies.yaml`):
```yaml
    policies.yaml:
      apiVersion: 1
      policies:
        - orgId: 1
          receiver: slack-homelab
          group_by:
            - grafana_folder
            - alertname
          group_wait: 3m
          group_interval: 10m
          repeat_interval: 6h
          routes:
            - receiver: slack-homelab
              group_by:
                - alertname
                - instance
              object_matchers:
                - ["severity", "=~", "critical|warning"]
              group_wait: 3m
              group_interval: 10m
              repeat_interval: 6h
              routes:
                - receiver: slack-homelab
                  object_matchers:
                    - ["severity", "=", "critical"]
                  repeat_interval: 1h
```

> **Note:** Root receiver is `slack-homelab` (not `grafana-default-email`). This ensures unmatched alerts still go to Slack rather than a dead-letter email receiver.

**Alert rules** (`rules.yaml`):

18 rules across 3 folders (17 original + 1 watchdog). Each rule follows this pattern:

```yaml
- uid: <kebab-case-uid>
  title: <AlertName>
  condition: C
  data:
    - refId: A
      relativeTimeRange:
        from: <see note below>
        to: 0
      datasourceUid: thanos
      model:
        expr: '<PromQL>'
        refId: A
    - refId: B
      datasourceUid: __expr__
      model:
        type: reduce
        expression: A
        reducer: last
        refId: B
    - refId: C
      datasourceUid: __expr__
      model:
        type: threshold
        expression: B
        conditions:
          - evaluator:
              type: <lt|gt>
              params: [<value>]
        refId: C
  for: <duration>
  noDataState: NoData
  execErrState: <Error or Alerting — see table>
  labels:
    severity: <warning|critical>
  annotations:
    summary: "<message with {{ $labels.instance }} etc.>"
```

**`relativeTimeRange.from` must match the PromQL range selector:**
- Instant queries (`up`, gauge values): `from: 600` (10 min)
- `rate(...[10m])`: `from: 600` (10 min)
- `predict_linear(...[6h], ...)`: `from: 21600` (6 hours)
- `increase(...[24h])`: `from: 86400` (24 hours)

If `relativeTimeRange.from` is shorter than the PromQL range, the query won't have enough data and will return empty results or wrong values.

**`execErrState` policy:**
- `Alerting` only for critical data-integrity rules (SmartDiskUnhealthy, ZfsPoolFaulted, ZfsPoolUnavail, SmartNvmeCriticalWarning) — better a false positive than a missed disk failure
- `Error` for all other rules — a Thanos outage should show error state in Grafana UI, not flood Slack with 17 false alerts

Rules to create:

| # | Folder (eval interval) | UID | Title | PromQL | relativeTimeRange.from | Threshold | For | Sev | execErrState |
|---|------------------------|-----|-------|--------|------------------------|-----------|-----|-----|--------------|
| 1 | Infra Node Health (60s) | infra-host-down | InfraHostDown | `up{job="integrations/unix",source="infra"}` | 600 | < 1 | 5m | critical | Error |
| 2 | | fs-space-low | FilesystemSpaceLow | `(1 - node_filesystem_avail_bytes{fstype!~"tmpfs\|overlay",source="infra"} / node_filesystem_size_bytes{fstype!~"tmpfs\|overlay",source="infra"})` | 600 | > 0.85 | 15m | warning | Error |
| 3 | | fs-space-critical | FilesystemSpaceCritical | (same as above) | 600 | > 0.95 | 5m | critical | Error |
| 4 | | fs-fill-24h | FilesystemWillFillIn24h | `predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs\|overlay",source="infra"}[6h], 24*3600)` | 21600 | < 0 | 1h | warning | Error |
| 5 | | high-memory | HighMemoryUsage | `(1 - node_memory_MemAvailable_bytes{source="infra"} / node_memory_MemTotal_bytes{source="infra"})` | 600 | > 0.90 | 15m | warning | Error |
| 6 | | high-cpu | HighCpuLoad | `(1 - avg by(instance)(rate(node_cpu_seconds_total{mode="idle",source="infra"}[10m])))` | 600 | > 0.90 | 30m | warning | Error |
| 7 | Infra SMART Health (60s) | smart-unhealthy | SmartDiskUnhealthy | `smartctl_device_smart_status` | 600 | < 1 | 5m | critical | Alerting |
| 8 | | smart-realloc | SmartReallocatedSectorsGrowing | `increase(smartctl_device_attribute{attribute_name="Reallocated_Sector_Ct",attribute_value_type="raw"}[24h])` | 86400 | > 0 | 5m | warning | Error |
| 9 | | smart-pending | SmartPendingSectorsGrowing | `increase(smartctl_device_attribute{attribute_name="Current_Pending_Sector",attribute_value_type="raw"}[24h])` | 86400 | > 0 | 5m | warning | Error |
| 10 | | smart-temp-high | SmartDiskTemperatureHigh | `smartctl_device_temperature{temperature_type="current"}` | 600 | > 60 | 10m | warning | Error |
| 11 | | smart-temp-crit | SmartDiskTemperatureCritical | `smartctl_device_temperature{temperature_type="current"}` | 600 | > 65 | 5m | critical | Error |
| 12 | | smart-nvme-media | SmartNvmeMediaErrors | `increase(smartctl_device_media_errors[24h])` | 86400 | > 0 | 5m | warning | Error |
| 13 | | smart-nvme-crit | SmartNvmeCriticalWarning | `smartctl_device_critical_warning` | 600 | > 0 | 5m | critical | Alerting |
| 14 | | smart-exporter-down | SmartExporterDown | `up{job="smartctl"}` | 600 | < 1 | 5m | warning | Error |
| 15 | Infra ZFS Health (30s) | zfs-degraded | ZfsPoolDegraded | `node_zfs_zpool_state{state="degraded",source="infra"}` | 600 | > 0 | 5m | critical | Error |
| 16 | | zfs-faulted | ZfsPoolFaulted | `node_zfs_zpool_state{state="faulted",source="infra"}` | 600 | > 0 | 1m | critical | Alerting |
| 17 | | zfs-unavail | ZfsPoolUnavail | `node_zfs_zpool_state{state="unavail",source="infra"}` | 600 | > 0 | 1m | critical | Alerting |
| 18 | Infra Watchdog (60s) | grafana-watchdog | GrafanaAlertingWatchdog | `vector(1)` | 600 | > 0 | 0s | none | Error |

> **ZFS folder uses 30s eval interval** to ensure `for: 1m` rules fire within 30-60s instead of 60-120s. These are data-loss conditions where faster detection matters.

> **Rule 18 (GrafanaAlertingWatchdog)** verifies the Grafana→Slack pipeline end-to-end. It fires continuously via `vector(1)`. Route it to Slack with a long repeat interval (24h) or to a heartbeat monitor. If you stop seeing this periodic notification, the pipeline is broken.

#### 1d. Raise SmartDiskTemperatureHigh threshold

In `additionalPrometheusRulesMap.infra-smart-health`:
- Change SmartDiskTemperatureHigh expr from `> 55` to `> 60` (line 270)

This keeps the Prometheus rule (which feeds HA) consistent with the Grafana rule (60C in the table above).

### Phase 2: Remove Alertmanager Slack receiver (after Phase 1 is verified)

In a **separate commit** after Phase 1 is deployed and verified working:

In the `alertmanager.config` section:
- **Remove** the `slack` receiver definition (lines 138-144)
- **Remove** the Slack route from `routes` (lines 129-132)
- **Remove** `alertmanagerSpec.secrets: [alertmanager-slack-webhook]` (line 148) — Alertmanager no longer needs this secret mounted
- **Keep** the `homeassistant` receiver and its routes unchanged
- **Keep** the `"null"` receiver for Watchdog
- **Keep** the `alertmanager-slack-webhook` K8s Secret resource (Grafana still needs it via `env.valueFrom`)

### Phase 3: Documentation

Update `docs/monitoring.md`:
- Add a section about Grafana Alerting describing the Slack notification pipeline
- Note that HA webhook stays in Alertmanager
- Document that Prometheus rules and Grafana rules must have matching thresholds
- Note rule UID stability requirement (changing a UID resets that rule's firing state)

## File Summary

| File | Phase | Action |
|------|-------|--------|
| `kubernetes/apps/argocd-apps/apps/kube-prometheus-stack.yaml` | 1 | Add `grafana.ini`, `env`, `grafana.alerting` sections; raise temp threshold |
| `kubernetes/apps/argocd-apps/apps/kube-prometheus-stack.yaml` | 2 | Remove AM Slack receiver, route, and secret mount |
| `docs/monitoring.md` | 3 | Document Grafana Alerting pipeline |
| `.gitignore` | Pre | Add `mcp.json` / `.mcp.json` entries |

## Deployment

### Phase 1 deployment
1. Commit + push + create PR (Phase 1 changes only)
2. Merge PR
3. `flux reconcile kustomization apps-config` (update ArgoCD Application CRD)
4. Wait for ArgoCD to sync kube-prometheus-stack
5. If ArgoCD shows stale, hard refresh: `kubectl patch application kube-prometheus-stack -n argocd --type merge -p '{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}'`
6. Verify (see Verification section below)

### Phase 2 deployment
7. After Phase 1 verification passes, commit Phase 2 changes (remove AM Slack)
8. Same Flux/ArgoCD reconcile flow
9. Verify AM no longer has Slack receiver

## Verification

### Phase 1 checks
1. **Grafana MCP**: Use `mcp-grafana` tools to list alert rules, contact points, and notification policies
2. **Grafana UI**: Visit `https://grafana.sharmamohit.com/alerting/list` — should show all 18 rules (17 + watchdog)
3. **Slack test**: Trigger a test notification from Grafana contact points page — verify links work (silence URL should point to `grafana.sharmamohit.com`, not `localhost`)
4. **Watchdog**: Verify GrafanaAlertingWatchdog fires and sends a Slack notification
5. **Temp threshold**: Confirm drives at 56C are no longer triggering SmartDiskTemperatureHigh

### Phase 2 checks
6. **No duplicate Slack**: Verify Alertmanager no longer has a Slack receiver:
   ```
   kubectl exec -n monitoring alertmanager-kube-prometheus-stack-alertmanager-0 -- \
     cat /etc/alertmanager/config_out/alertmanager.env.yaml | grep -A5 slack
   ```
7. **HA webhook intact**: Verify Home Assistant still receives alerts (check HA automation history)

## Review Findings (Security + Platform)

Findings from security-auditor and platform-engineer review that informed this plan:

### Incorporated (fixes applied above)

| # | Finding | Severity | Resolution |
|---|---------|----------|------------|
| 1 | `envValueFrom` is wrong key — use `env` with `valueFrom` | Critical | Fixed in step 1b |
| 2 | `relativeTimeRange` too short for `increase()` and `predict_linear()` | Critical | Fixed — table shows correct values per rule |
| 3 | `GF_SERVER_ROOT_URL` not set — Slack links point to localhost | High | Fixed in step 1a (`grafana.ini.server.root_url`) |
| 4 | `.DashboardURL` empty without explicit dashboard linkage | High | Replaced with static alerting list link in template |
| 5 | `execErrState: Alerting` causes alert storm on Thanos outage | Medium | Changed to `Error` for most rules, `Alerting` only for 4 critical data-integrity rules |
| 6 | Use `secureSettings` for webhook URL to prevent API exposure | Medium | Webhook URL moved to `secureSettings` in contact point |
| 7 | Root policy receiver was unconfigured `grafana-default-email` | Medium | Changed to `slack-homelab` |
| 8 | MCP service account over-privileged (Editor) | Medium | Changed to Viewer with 90-day token expiry |
| 9 | No Grafana alerting watchdog | Low | Added rule 18 (GrafanaAlertingWatchdog) |
| 10 | Deploy in two phases to avoid alerting gap | Low | Split into Phase 1 (add) and Phase 2 (remove AM Slack) |
| 11 | 60s eval + `for: 1m` = 60-120s detection for ZFS | Low | ZFS folder uses 30s eval interval |
| 12 | Add `mcp.json` to `.gitignore` | Low | Added to pre-step |

### Acknowledged (not addressed in this plan)

| # | Finding | Severity | Reason |
|---|---------|----------|--------|
| A1 | Grafana uses default admin password, no auth hardening | High | Pre-existing issue (Feb 2026 audit finding #11). Should be addressed separately — SSO/OIDC via PocketID or SOPS-encrypted admin password. Out of scope for this alerting migration. |
| A2 | No NetworkPolicy in monitoring namespace | Low | Homelab-acceptable risk. Would add defense-in-depth but is out of scope. |
| A3 | Dual evaluation creates threshold drift risk | Low | Documented in Phase 3. Future improvement: migrate HA to Grafana webhook contact point to eliminate dual evaluation. |
| A4 | Rule UID changes reset firing state | Medium | UIDs are stable kebab-case strings. Documented in Phase 3. |
| A5 | MCP token stored in plaintext in mcp.json | Low | Acceptable for local dev workstation. Mitigated by `.gitignore` and 90-day expiry. |
